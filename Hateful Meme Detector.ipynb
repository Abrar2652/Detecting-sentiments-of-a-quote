{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first place the attached `'balmy-channel-278201-7d1169cc888d.json'` file to your `'C:\\\\Users\\your_document\\'` as 'GOOGLE_APPLICATION_CREDENTIALS'. Then input the file_name and path_name of the test_data. It'll automatically evaluate sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hackerearth\\Test100.jpg\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'detect_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8f39995bc53f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hackerearth/*.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetect_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mFilename2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detect_text' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os, io\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision import types\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']= 'Your API Key'\n",
    "client = vision_v1.ImageAnnotatorClient()\n",
    "\n",
    "Category=[]\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "import os\n",
    "pytesseract.pytesseract.tesseract_cmd=r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "def ocr_core(filename):\n",
    "    \"\"\"\n",
    "    This function will handle the core OCR processing of images.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #text = pytesseract.image_to_string(Image.open(filename))  # We'll use Pillow's Image class to open the image and pytesseract to detect the string in the image\n",
    "  \n",
    "   \n",
    "    sentiment_dict= analyser.polarity_scores(text) \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    # print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\") \n",
    "    #   print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\") \n",
    "    #   print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\") \n",
    "    \n",
    "\n",
    "    if sentiment_dict['compound'] >= 0.08 : \n",
    "                Category.append('Positive')   \n",
    "                print('Positive')   \n",
    "  \n",
    "    elif (sentiment_dict['compound'] > - 0.08) & (sentiment_dict['compound'] < 0.08): \n",
    "            Category.append('Random')\n",
    "            print('Random')\n",
    "        \n",
    "    elif   (sentiment_dict['compound'] <= -0.08):\n",
    "                Category.append('Negative')\n",
    "                print('Negative')\n",
    "  \n",
    "    #return text\n",
    "Filename2=[]\n",
    "for file in glob.glob(\"Hackerearth/*.jpg\"):\n",
    "    print(file)\n",
    "    print(detect_text(file))\n",
    "    Filename2.append(file)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=text.lower().split()\n",
    "    from nltk.corpus import stopwords\n",
    "#    stops=set(stopwords.words('english'))\n",
    "#    text=[w for w in text if not w in stops]\n",
    "    \n",
    "    \n",
    "    text= \" \".join(text)\n",
    "    \n",
    "    text=re.sub(r'https?://[A-Za-z0-9./]+','url',text)\n",
    "    text=re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\",\" \",text)\n",
    "    text=re.sub(r\"what's\",\"what is\",text)\n",
    "    text=re.sub(r\"\\'s\",\" is \" ,text)\n",
    "    text=re.sub(r\"\\'ve\",' have ',text)\n",
    "    text=re.sub(r\"n't\",' not ',text)\n",
    "    text=re.sub(r\"i'm\",'i am',text)\n",
    "    text=re.sub(r\"\\'re\",' are ',text)\n",
    "    text=re.sub(r\"\\'d\",' would ',text)\n",
    "    text=re.sub(r\"\\'ll\",' will ',text)\n",
    "    text=re.sub(r\"\\n\",'',text)\n",
    "    text=re.sub(r',',',',text)\n",
    "    text=re.sub(r'\\.','.',text)\n",
    "    text=re.sub(r'!','!',text)\n",
    "    text=re.sub(r'\\/',\" \",text)\n",
    "    text=re.sub(r'\\^',' ^ ',text)\n",
    "    text=re.sub(r'\\=',' = ',text)\n",
    "    text=re.sub(r\"'\",' ',text)\n",
    "    text=re.sub(r'(\\d+)(k)',r\"\\g<1>000\",text)\n",
    "    text=re.sub(r':',' : ',text)\n",
    "    text=re.sub(r' e g ',' eg ',text)\n",
    "    text=re.sub(r' b g ',' bg ',text)\n",
    "    text=re.sub(r' u s ',' american ',text)\n",
    "    text=re.sub(r'\\0s','0',text)\n",
    "    text=re.sub(r' 9 11 ','911',text)\n",
    "    text=re.sub(r'[0123456789]','',text)\n",
    "    text=re.sub(r'e - mail','email',text)\n",
    "    text=re.sub(r'j k','jk',text)\n",
    "    text=re.sub(r'\\s{2,}',' ',text)\n",
    "    text=re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "    text=re.sub(r'(\\w)\\1{2,}',r'\\1\\1',text)\n",
    "    text=re.sub(r'\\w(\\w)\\1{2}','',text)\n",
    "   \n",
    "    \n",
    "    return text\n",
    "\n",
    "def del_NoAlphaWords(sentence):\n",
    "    return \" \".join([word for word in sentence.split() if word.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when people ask what i see in you, i just smile and look away because i am afraid if they knew, they would fall in love with you too.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, io\n",
    "import re\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "Category=[]\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from google.cloud import language\n",
    "#from google.cloud.language import enums\n",
    "#from google.cloud.language import types\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']= 'API key'\n",
    "client = vision.ImageAnnotatorClient()\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def detect_text(img):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    \n",
    "    with io.open(img, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "    response = client.text_detection(image=image)  # returns TextAnnotation\n",
    "    df = pd.DataFrame(columns=['description'])\n",
    "    texts = response.text_annotations\n",
    "    for text in texts:\n",
    "            df = df.append(\n",
    "                dict(\n",
    "                    \n",
    "                    description= clean_text  (text.description)\n",
    "                ),\n",
    "                ignore_index=True\n",
    "            )\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    try:\n",
    "        text= (df['description'][0])\n",
    "        text = porter.stem(text)\n",
    "    except IndexError:\n",
    "        text = 'i am neutral'\n",
    " #   print (analyze(text))\n",
    "    \n",
    "        \n",
    "  #  print(df['description'])\n",
    "    print(text)\n",
    "    if len (text.split())<3:\n",
    "          text = 'i am neutral'\n",
    "\n",
    "    sentiment_dict= analyze2(text) \n",
    "    if sentiment_dict >= 0.008: \n",
    "                    Category.append('Positive')   \n",
    "                    return('Positive')   \n",
    "\n",
    "    elif (sentiment_dict > - 0.008) & (sentiment_dict < 0.008): \n",
    "            Category.append('Random')\n",
    "            return('Random')\n",
    "\n",
    "    elif   (sentiment_dict <= -0.008):\n",
    "            Category.append('Negative')\n",
    "            return('Negative')\n",
    "   #best 0.08 \n",
    "         \n",
    "\n",
    "    \n",
    "    \n",
    "file='Test1001.jpg'    \n",
    "path=  \"C://Users/Abrar/Hackerearth/\"    \n",
    "detect_text(os.path.join(path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions, SyntaxOptions\n",
    "from ibm_watson import ApiException\n",
    "def analyze2(text):\n",
    "    \n",
    "        authenticator = IAMAuthenticator('API key')\n",
    "        natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "            version='2019-11-28',\n",
    "            authenticator=authenticator\n",
    "        )\n",
    "\n",
    "        natural_language_understanding.set_service_url('url')\n",
    "        try:\n",
    "            response = natural_language_understanding.analyze(\n",
    "                text=text,\n",
    "\n",
    "                features=Features(sentiment=SentimentOptions())).get_result()\n",
    "\n",
    "            texts = response\n",
    "            texts=pd.DataFrame(texts.items())[1][1]\n",
    "            texts=pd.DataFrame(texts.items())[1][0]\n",
    "            x=pd.DataFrame(texts.items())[1][0]\n",
    "            return x \n",
    "        except ApiException:\n",
    "            sentiment=0.3\n",
    "            return sentiment\n",
    "analyze2('i am neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-cloud-language in h:\\anaconda\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.14.0 in h:\\anaconda\\lib\\site-packages (from google-cloud-language) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (49.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (1.52.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.18.0 in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (1.18.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in h:\\anaconda\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (1.30.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in h:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in h:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in h:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in h:\\anaconda\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in h:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in h:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in h:\\anaconda\\lib\\site-packages (from google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in h:\\anaconda\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2.0dev,>=1.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-language) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.699999988079071"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Demonstrates how to make a simple call to the Natural Language API.\"\"\"\n",
    "\n",
    "import argparse\n",
    "from google.api_core.exceptions import InvalidArgument\n",
    "\n",
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "from google.cloud.language_v1 import types\n",
    "from google.cloud.language_v1 import language_service_client\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']= 'API key'\n",
    "\n",
    "def print_result(annotations):\n",
    "    score = annotations.document_sentiment.score\n",
    "    magnitude = annotations.document_sentiment.magnitude\n",
    "\n",
    "    for index, sentence in enumerate(annotations.sentences):\n",
    "        sentence_sentiment = sentence.sentiment.score\n",
    "        print('{}'.format(\n",
    "            sentence_sentiment))\n",
    "\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def analyze(text):\n",
    "    \"\"\"Run a sentiment analysis request on text within a passed filename.\"\"\"\n",
    "    client = language_service_client.LanguageServiceClient()\n",
    "\n",
    "  #  with open(movie_review_filename, 'r') as review_file:\n",
    "     # Instantiates a plain text document.\n",
    "    \n",
    "  #  content = text.read()\n",
    "    content=text\n",
    "    document = language_v1.types.Document(\n",
    "        content=content,\n",
    "        type=enums.Document.Type.PLAIN_TEXT,\n",
    "        language='en'\n",
    "    )\n",
    "      #  type='PLAIN_TEXT',\n",
    "  #  )\n",
    "    \n",
    "    try:\n",
    "        response = client.analyze_sentiment(\n",
    "                document=document,\n",
    "                encoding_type='UTF32',\n",
    "            )\n",
    "        sentiment = response.document_sentiment\n",
    "        return (sentiment.score)\n",
    "    except InvalidArgument:\n",
    "        sentiment=0.0\n",
    "        return sentiment\n",
    "    \n",
    "  #  annotations = client.analyze_sentiment(document=document)\n",
    "  # score = annotations.document_sentiment.score\n",
    "  #  print(print_result(annotations))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "analyze(\"the notion that i should be fine with the status quo even if iam not wholly affected by the status quo is repulsive.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck you guys @123123\n"
     ]
    }
   ],
   "source": [
    "tokens=('fuck you guys @123123' )   \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = porter.stem(tokens)\n",
    "print(stemmed)\n",
    "\n",
    "tokens=('fuck you guys @123123' )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-ca781a78f4aa>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-ca781a78f4aa>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    if sentiment_dict >= 0.05 :\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "  sentiment_dict= analyze(text) \n",
    "    if sentiment_dict >= 0.05 : \n",
    "                Category.append('Positive')   \n",
    "                return('Positive')   \n",
    "  \n",
    "    elif (sentiment_dict > - 0.05) & (sentiment_dict < 0.05): \n",
    "            Category.append('Random')\n",
    "            return('Random')\n",
    "        \n",
    "    elif   (sentiment_dict <= -0.05 ):\n",
    "                Category.append('Negative')\n",
    "                return('Negative')\n",
    "ar,zh,zh-Hant,nl,en,fr,de,id,it,ja,ko,pl,pt,es,th,tr,vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20000000298023224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']= 'API key'\n",
    "def sample_analyze_sentiment(text_content):\n",
    "\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "\n",
    "    language= 'en'\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "   \n",
    "    return(response.document_sentiment.score)\n",
    "    \n",
    "    # Get sentiment for all sentences in the document\n",
    " #   for sentence in response.sentences:\n",
    " #       print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    " #       print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    " #       print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    \n",
    "sample_analyze_sentiment(\"being gay is not a crime and it is not a sin. stop using god to justify your prejudice. religion is about loving one another. you are  just looking for an excuse to hate. being gay   proud quotes www.geckoandfly.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['Filename']=Filename2\n",
    "b['Category']=Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv('Test3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
